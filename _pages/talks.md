---
layout: archive
title: "Conference Presentations"
permalink: /talks/
author_profile: true
---

* <strong><u>[FPL 2020] A Domain-Specific Architecture for Accelerating Sparse Matrix Vector Multiplication on FPGAs: </u></strong> FPGAs allow custom memory hierarchy and flexible data movement with highly fine-grained control. These capabilities are critical for building high performance and energy efficient domain-specific architectures (DSAs), especially for workloads with irregular memory access and data-dependent communication patterns. Sparse linear algebra operations, especially sparse matrix vector multiplication (SpMV), are examples of such workloads and are becoming important due to their use in numerous areas of science and engineering. Existing FPGA-based DSAs for SpMV do not allow customization through plug and play of the building blocks. For example, most of these DSAs require switching network/crossbar architecture as a building block for routing matrix data to banked vector memory blocks. In this paper, we first present an approach where a custom network is built using simple blocks arranged in a regular fashion to exploit low-level architecture details. Further, we make use of this network to replace expensive crossbars employed in GEMX SpMV engine and develop an end-to-end tool-flow around mixed IP approach (HLS/RTL). Due to the modularity of our design, our tool-flow allows us to insert an additional block in the design to guarantee zero-stall from the accumulation stage. On Alveo U200, we report performance numbers of up to 4.4 GFLOPS (92% peak bandwidth utilization) using our accelerator (attached with one DDR4). <br>
<iframe width="410" height="215" src="https://www.youtube.com/embed/ascaEvDKpWM" title="A Domain-Specific Architecture for Accelerating Sparse Matrix Vector Multiplication on FPGAs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>

* <strong><u>[SLIP 2020] Role of on-chip networks in building domain-specific architectures (DSAs) for sparse computations: </u></strong> DSAs for machine learning (ML) such as Google TPU, Microsoft Brainwave, Xilinx xDNN are becoming prominent because of high energy-efficiency and performance. These DSAs perform dense linear algebra efficiently by minimizing data movement, exploiting high data reuse, regular memory access pattern and data locality (temporal and spatial). DSAs for domains like graph analytics and HPC are emerging at at rapid pace as well where most of the computations revolve around sparse linear algebra, specifically Sparse Matrix Vector Multiplication (SpMV). SpMV refers to the multiplication of a sparse matrix A by a dense vector x to produce a result vector y. Designing high performance and energy efficient DSs for SpMV is challenging due to highly irregular and random memory access patterns, prro temporal and spatial locality and very low data reuse opportunites. SpMV DSs explorit distributed onchip memory blocks to store vector entries for avoiding off-chip random memory access. However, a switching network architecture or a crossbar is usually required as a building block for routing matrix no-zero elements to on-chip memory blockjs. In thus presentation, we will discuss about the network architectures and switches employed in most of the SpMV DSAs, our SpMV DSA based on 2D-mech network, design choices for the FPGA implementation of the DSA, and scalability aspects. In more detail for our use-case, we will highlight the importance and challenges of achieving energy efficient data movement using scalable on-chip network architectures. <br>
<iframe width="410" height="215" src="https://www.youtube.com/embed/CrWcchvFJVM?start=19313" title="Role of on-chip networks in building domain-specific architectures (DSAs) for sparse computations" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br>
